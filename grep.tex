\documentclass{beamer}
\usepackage{ctex, hyperref}
\usepackage[T1]{fontenc}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}

\author{Lingching Tung}
\title{Grep: A Graph Learning Based Database Partitioning System}
\subtitle{}
\institute{Intelligence Data Storage And Management Laboratory}
\date{July 13, 2023}
\usepackage{Tsinghua}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}


\begin{document}

\kaishu
\begin{frame}
    \titlepage
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[width=0.2\linewidth]{pic/Huazhong_University_of_Science_&_Technology_logo.eps}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Background}
\begin{frame}
    \begin{itemize}[<+-| alert@+>]
        \item Existing distributed databases rely on users to specify partitioning keys in order to allocate the data to multiple compute nodes.

        \item It calls for an automatic database partitioning method based on both the data distribution and query patterns, whereas problem has been proven to be NP-hard.{\footnote{To simplify the problem, many existing methods rely on single primary keys for partitioning tables, which could be less effective in some scenarios.}}

        \item It is costly to really partition the databases and then evaluate the performance. The state-of-the-art cost model takes single queries as input and cannot encode the data and query distribution under selected partition keys and causes unreliable estimation.{\footnote{Deep reinforcement learning based methods require to evaluate the performance by really partitioning the data}}
    \end{itemize}
\end{frame}

\begin{frame}{Partitioning Challenges}
    \begin{itemize}
        \item Efficiently capturing intricate partitioning factors
        \item Effective selection of partitioning keys for performance optimization
        \item Evaluation of partitioning performance
    \end{itemize}
\end{frame}


\section{Overview}
\begin{frame}{Partitioning Workflow}
    \begin{itemize}
        \item Step 1: Train Evaluation Model to predict partitioning strategy performance.
    \end{itemize}
    \begin{figure}[htpb]
        \centering
        \includegraphics[width=1.03\linewidth]{pic/fig2.png}
        \tiny{Fig. 1. The Grep Overview.}
    \end{figure}
\end{frame}

\begin{frame}{Partitioning Workflow}
    \begin{itemize}
        \item Step 2: Build Column2Graph Model based on data and query features.
    \end{itemize}
    \begin{figure}[htpb]
        \centering
        \includegraphics[width=1.03\linewidth]{pic/fig2.png}
        \tiny{Fig. 1. The Grep Overview.}
    \end{figure}
\end{frame}

\begin{frame}{Partitioning Workflow}
    \begin{itemize}
        \item Step 3: Train Partitioning Model using the graph model to select partitioning keys.
    \end{itemize}
    \begin{figure}[htpb]
        \centering
        \includegraphics[width=1.03\linewidth]{pic/fig2.png}
        \tiny{Fig. 1. The Grep Overview.}
    \end{figure}
\end{frame}

\begin{frame}{Partitioning Workflow}
    \begin{itemize}
        \item Step 4: Use Evaluation Model to evaluate the selected keys and provide feedback to the Column2Graph Model and Partitioning Model.
    \end{itemize}
    \begin{figure}[htpb]
        \centering
        \includegraphics[width=1.03\linewidth]{pic/fig3.png}
        \tiny{Fig. 1. The Grep Overview.}
    \end{figure}
\end{frame}

\section{Column2Graph}
\subsection{Define the Model}
\begin{frame}[fragile]{Define the Model (1): Vertex Feature Vector}
    \begin{minipage}{1.0\linewidth}
\begin{lstlisting}[language=Python]
class Column:
  def __init__(self, _id, size, selectivity,
                length, num_f, num_a, num_w):
    self.table_id = _id
    self.table_size = size
    self.tuple_selectivity = selectivity
    self.tuple_length = length
    self.num_filters = num_f
    self.num_aggregates = num_a
    self.num_writes = num_w
\end{lstlisting}
    \end{minipage}\hspace{0cm}
    \begin{minipage}{1.0\linewidth}
        \begin{itemize}
            \item Encode the features of the vertex as a vector, which contains 7 dimensions
            \begin{itemize}
                \item #filters: number of filter operations w.r.t. the column
                \item #writes: number of write operations w.r.t the column
                \item #aggregates: number of aggregates w.r.t. the column
            \end{itemize}
        \end{itemize}
    \end{minipage}
\end{frame}

\begin{frame}[fragile]{Define the Model (2): Attention-based Vertex Feature Vector}
    \begin{exampleblock}{Benefits}
        \begin{itemize}
            \item Get the importance of each vertex and enables Grep to prioritize important vertices.
            \item Better reflect performance requirements by learning different encodings of the graph features.
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Attention Network}
        \vspace{0.25cm}
        \begin{minipage}{1.0\linewidth}
\begin{lstlisting}[language=Python]
def __init__(self, input_size):
    super(AttentionNetwork, self).__init__()
    self.dense_layer = nn.Linear(input_size, 1)
\end{lstlisting}
    \end{minipage}
        \begin{equation*}
            V(G) = \sum_{c_j\in{G}} a(c_j) V(c_j)
        \end{equation*}
    \end{exampleblock}
\end{frame}

\begin{frame}{Define the Model (3): Edge Weight}
    \begin{exampleblock}{Frequencies}
        \begin{itemize}
            \item What: the number of join predicates containing the columns.
            \item Why: more remote joins can be reduced by using the frequently joined column as partitioning keys.
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Cardinalities}
        \begin{itemize}
            \item What: sample tuples from the tables and estimate it by executing predicates on the sampled tuples.
            \item \texttt{sql = "SELECT COUNT(*) FROM \{\} JOIN \{\} ON \{\}".format(lt, rt, predicate)}
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Weight Computation}
        \begin{itemize}
            \item for any edge $\mathrm{(u, v)}$,
                $\mathrm{W(u, v) = \sum_{q} f(q) Card(u, v, q)}$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Define the Model (4): Attention-based Edge Feature Vector}
    \begin{exampleblock}{Attention Network}
        \begin{align}
            \psi_{c_j} &= \mathrm{Sigmoid}(V(c_j))\\
            \mu_{i\to j}(&c_j) = W(c_i,c_j) \psi_{c_i}\psi_{c_j}
        \end{align}
        \begin{align}
            a(c_j) \propto \exp (\hat{\omega}_{jj}\psi_{c_j} + \omega_{jj}\mu_{j\to j}(c_j) + \sum_{c_i} \omega_{ij}\mu_{i\to j}(c_j))
        \end{align}
        \begin{itemize}
            \item ${\omega}_{jj}$ is the network weight, and $\hat{\omega}_{jj}$ is the unit vector of $w_{jj}$;
            \item $\psi_{c_j}$ is the normalized vertex features; $\mu_{i\to j}(c_j)$ is the edge feature vector where $c_i$ is any adjacent vertex of $c_j$ and $W(c_i, c_j)$ is the edge weight.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]{Define the Model (4): Attention-based Edge Feature Vector}
    \begin{minipage}{1.0\linewidth}
\begin{lstlisting}[language=Python]
def forward(self, vertex_matrix, adj_matrix):
  vertex_n = torch.sigmoid(vertex_matrix)
  
  mu = torch.matmul(adj_matrix, vertex_n)
  mu_self = torch.diag(mu)
  mu_sum = torch.sum(mu, dim=1) - mu_self
  
  attention_logits = 
      self.dense_layer(vertex_n)
  attention_logits += 
      mu_self.view(-1, 1) + mu_sum.view(-1, 1)
  attention_weights =
      F.softmax(attention_logits, dim=1)

  return attention_weights
\end{lstlisting}
    \end{minipage}    
\end{frame}

\subsection{Example}
\begin{frame}{Example}
    \begin{figure}[htpb]
        \centering
        \includegraphics[width=1.08\linewidth]{pic/fig1.png}
        \tiny{Fig. 2. Column Graph Construction.}
    \end{figure}
\end{frame}

\section{Partitioning Model}
\subsection{Architecture}
\begin{frame}{Architecture}
    Grep use graph model to select partitioning keys.

    \begin{exampleblock}{Input}
        \begin{itemize}
            \item the graph model in the form of vertex matrix and edge matrix
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Output}
        \begin{itemize}
            \item selected columns
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Components}
        \begin{itemize}
            \item Graph Embedding
            \item Partitioning-Key Selection
        \end{itemize}
    \end{exampleblock}
\end{frame}

\subsection{Graph Embedding}
\begin{frame}{Graph Embedding}
    \begin{itemize}
        \item Graph Neural Network includes a graph embedding layer and an activation function $ReLU$:
        \begin{itemize}
            \item The graph embedding layer encodes the join patterns via graph convolution;
            \item The $ReLU$ layer converts the embedded columns into a fixed-length vector
        \end{itemize}
        \item For each column $c_i$ , we denote the graph structures as 
            \begin{align}
                G(c_i ) = (V (c_i ), D(c_i ), E)\nonumber
            \end{align}
            \begin{itemize}
                \item $V (c_i )$ is the column feature vector;
                \item $D(c_i )$ is the neighborhood matrix with each row representing the feature vector of a joined column of $c_i$;
                \item $E$ is the edge matrix
            \end{itemize}
        \item We need to embed these three factors into an embedding vector $H (c_i )$.
    \end{itemize}
\end{frame}

\subsection{Graph Embedding Algorithm}
\begin{frame}{Graph Embedding Algorithm (1)}
    \begin{minipage}{0.4\linewidth}
        \begin{figure}[htpb]
            \centering
            \includegraphics[width=1\linewidth]{pic/fig4.png}
            \tiny{Fig. 3. Graph Embedding.}
        \end{figure}
    \end{minipage}%
    \begin{minipage}{0.65\linewidth}
        \begin{itemize}
            \item Step 1 - Represent Graph Structure.
        \end{itemize}
    \end{minipage}
\end{frame}

\begin{frame}{Graph Embedding Algorithm (2)}
    \begin{minipage}{0.4\linewidth}
        \begin{figure}[htpb]
            \centering
            \includegraphics[width=1\linewidth]{pic/fig4.png}
            \tiny{Fig. 3. Graph Embedding.}
        \end{figure}
    \end{minipage}%
    \begin{minipage}{0.65\linewidth}
        \begin{itemize}
            \item Step 2 - Embed Neighbor Information.
        \end{itemize}
    \end{minipage}
\end{frame}

\begin{frame}{Graph Embedding Algorithm (3)}
    \begin{minipage}{0.4\linewidth}
        \begin{figure}[htpb]
            \centering
            \includegraphics[width=1\linewidth]{pic/fig4.png}
            \tiny{Fig. 3. Graph Embedding.}
        \end{figure}
    \end{minipage}%
    \begin{minipage}{0.65\linewidth}
        \begin{itemize}
            \item Step 3 - Embed into Fixed-Length Vector.
        \end{itemize}
    \end{minipage}
\end{frame}

\subsection{Key Selection}
\begin{frame}{Key Selection}
    Utilize Taylor decomposition to reversely extract the relevance of each column to the embeddings in the Graph Neural Network and then select keys based on the relevance distribution
    \vspace{0.3cm}
    \begin{itemize}
        \item Why: it is hard to directly select columns from the embedding vectors extracted from graph features by Graph Neural Network, which are high dimensional and require further training to capture correlations.
        \vspace{0.15cm}
        \item What: includes two parts
            \begin{itemize}
                \item Taylor decomposition;
                \item Binary classification: It inputs the partitioning benefits of all the columns and outputs the selection keys with multinomial logistic regression, where 1 denotes a column is selected and 0 otherwise.
            \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Key Selection Algorithm}
\begin{frame}[fragile]{Key Selection Algorithm (1)}
    \begin{minipage}{1.045\linewidth}
\begin{lstlisting}[language=Python]
def forward(self, graph):
    padded_vertex_matrix, padded_edge_matrix = 
        self.padding_graph_matrices(
            graph.vertex_matrix, graph.edge_matrix)
    origin_graph = graph
    embed = padded_vertex_matrix
    for gnn in self.gnns:
        embed, V =
            gnn(embed, padded_edge_matrix)
    total_r = torch.ones(embed.shape[0])
\end{lstlisting}
    \end{minipage}
\end{frame} 

\begin{frame}[fragile]{Key Selection Algorithm (2)}
    \begin{minipage}{1.045\linewidth}
\begin{lstlisting}[language=python]
for gnn in reversed(self.gnns):
    gnn_weights = gnn.W[:embed.size(-1), :]
    partial_derivatives = gnn_weights.t() * total_r

relevance =
    partial_derivatives[:embed.size(-1), :] * embed

total_r = relevance.sum(dim=1)
benefits =
    total_r[:origin_graph.vertex_matrix.size(0)]

probabilities =
    torch.exp(benefits) / (1 + torch.exp(benefits))

partitioning_keys = (probabilities > 0.5).int()
return partitioning_keys
\end{lstlisting}
    \end{minipage}
\end{frame}

\section{Evaluation Model}
\subsection{Model Overview}
\begin{frame}{Model Overview}
    \begin{itemize}
        \item Generate a k-node sample graph, where the vertices are sampled tuples and the edges are join correlations between tuples, and k denotes the node number. Then utilize graph learning to estimate the partitioning performance using the k-node sample graph.
    \end{itemize}
    \vspace{0.35cm}
    \begin{minipage}{1.0\linewidth}
        \begin{figure}[htpb]
            \centering
            \includegraphics[width=1\linewidth]{pic/fig6.png}
            \tiny{Fig. 4. Evaluation Model.}
        \end{figure}
    \end{minipage}
\end{frame}

\subsection{Evaluation Algorithm}
\begin{frame}{Evaluation Algorithm (1)}
    \begin{exampleblock}{Step 1 - Representation of Single Node.}
        \begin{itemize}
            \item Aggregate the features within the subgraph of each node.
            \item Embed the node into k embedding vectors $H(P_i)$ by multiplying the edge weight matrix, vertex matrix of each node with the globally-shared graph network weights.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Evaluation Algorithm (2)}
    \begin{exampleblock}{Step 2 - Representation of Multiple Nodes.}
        \begin{itemize}
            \item Generate a k-vertex graph
            \vspace{0.1cm}
            \item Conduct convolution on $\widetilde{V}$ and $\widetilde{E}$ to embrace remote join relations into partitioning vectors, denoted as $H (P) = \widetilde{E} ∗ \widetilde{V} · \widetilde{\omega}$, where $\widetilde{\omega}$ is the network weight
            \vspace{0.1cm}
            \item Conduct max pooling on $H (P)$ to pool up the partitioning vectors into a graph vector to represent the overall query costs within the k-node sample graph.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]{Evaluation Algorithm (2)}
    \begin{minipage}{1.04\linewidth}
\begin{lstlisting}[language=python]
def multiple_nodes(self, v_matrix, edge_matrix):
    # Generate k-vertex graph
    # +with compound vertex matrix and edge matrix
    tilde_v = torch.cat(v_matrix, dim=0)
    tilde_e = edge_matrix

    # Convolution on tilde_v and tilde_e
    h_p = torch.matmul(tilde_e, tilde_v)
    return h_p

def max_pooling(self, h_p):
    graph_vector = torch.max(
        h_p, dim=0)[0].unsqueeze(0)
    return graph_vector
\end{lstlisting}
    \end{minipage}
\end{frame}

\begin{frame}[fragile]{Evaluation Algorithm (3)}
    \begin{exampleblock}{Step 3 - Query Performance Mapping.}
        \begin{itemize}
            \item Use a fully-connected layer, with ReLU as the activation function, to map the graph vector on sampled tuples into the partitioning performance on the whole dataset.
        \end{itemize}
    \end{exampleblock}
    \vspace{0.15cm}
    \begin{minipage}{1.04\linewidth}
\begin{lstlisting}[language=python]
def query_performance_mapping(self, graph_vector):
    fc_layer = nn.Sequential(
        nn.Linear(graph_vector.size(1), 1),
        nn.ReLU()
    )

    performance = fc_layer(graph_vector)
    return performance
\end{lstlisting}
    \end{minipage}
\end{frame}

% \section{Experiments}

%\section{References}

% \begin{frame}[allowframebreaks]
%     \bibliography{ref}
%     \bibliographystyle{alpha}
%     % 如果参考文献太多的话，可以像下面这样调整字体：
%     % \tiny\bibliographystyle{alpha}
% \end{frame}

\begin{frame}
    \begin{center}
        {\Huge\calligra Thanks for your attention!}
        \vspace{1cm}

        {\Huge Q \& A}
    \end{center}
\end{frame}

\end{document}